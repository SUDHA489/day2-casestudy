{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1e3881-394b-4f5f-a735-b09cff8aa709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a testing code for notebook\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a testing code for notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ee5b93-c370-4688-81dc-e940b647bd3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydeequ\n  Downloading pydeequ-1.5.0-py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: numpy>=1.14.1 in /databricks/python3/lib/python3.12/site-packages (from pydeequ) (1.26.4)\nRequirement already satisfied: pandas>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from pydeequ) (1.5.3)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.12/site-packages (from pandas>=0.23.0->pydeequ) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas>=0.23.0->pydeequ) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=0.23.0->pydeequ) (1.16.0)\nDownloading pydeequ-1.5.0-py3-none-any.whl (37 kB)\nInstalling collected packages: pydeequ\nSuccessfully installed pydeequ-1.5.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "778e0653-deb7-4071-8b5d-2496ae3282e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "172e7623-9465-4352-abb7-df68af4cb99d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_VERSION\"] = \"3.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f74e6793-92a8-4a64-b2fc-0ceb0c301379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+----------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\n|check              |check_level|check_status|constraint                                                                                                                  |constraint_status|constraint_message|\n+-------------------+-----------+------------+----------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\n|Data quality checks|Error      |Success     |ComplianceConstraint(Compliance(sales_amount is non-negative,COALESCE(CAST(sales_amount AS DECIMAL(20,10)), 0.0) >= 0,None))|Success          |                  |\n+-------------------+-----------+------------+----------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\n\nVerification status: Success\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/dataframe.py:163: UserWarning: DataFrame constructor is internal. Do not directly use it.\n  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import VerificationSuite, VerificationResult\n",
    "\n",
    "# SparkSession is already available as `spark` in Databricks\n",
    "\n",
    "# Sample data\n",
    "data = spark.createDataFrame([\n",
    "    (1, 100),\n",
    "    (2, 200),\n",
    "    (3, 300),\n",
    "    (4, None)\n",
    "], [\"transaction_id\", \"sales_amount\"])\n",
    "\n",
    "# Define checks\n",
    "check = Check(spark, CheckLevel.Error, \"Data quality checks\") \\\n",
    "    .isNonNegative(\"sales_amount\")\n",
    "\n",
    "# Run verification\n",
    "result = VerificationSuite(spark).onData(data).addCheck(check).run()\n",
    "\n",
    "# Convert results to DataFrame\n",
    "result_df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "# Overall status\n",
    "print(\"Verification status:\", result.status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13955820-8af0-4e50-a062-1a3980ad75d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/dataframe.py:163: UserWarning: DataFrame constructor is internal. Do not directly use it.\n  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deequ checks passed\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import VerificationSuite\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import uuid\n",
    "from scipy.stats import ks_2samp\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import json\n",
    "\n",
    "# Spark session (already available in Databricks)\n",
    "spark = SparkSession.builder.appName(\"DataQualityChecks\").getOrCreate()\n",
    "\n",
    "# Load sample data with schema inference\n",
    "# data = spark.read.format(\"csv\") \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .option(\"inferSchema\", \"true\") \\\n",
    "#     .load(\"dbfs:/FileStore/shared_uploads/traininguser8@sudosu.ai/sample_sales_data-1.csv\")\n",
    "\n",
    "data = spark.table(\"hive_metastore.default.sample_sales_data\")\n",
    "\n",
    "# Define quality rules\n",
    "check = Check(spark, CheckLevel.Error, \"Data quality checks\") \\\n",
    "    .hasSize(lambda x: x >= 10) \\\n",
    "    .isNonNegative(\"sales_amount\")\n",
    "\n",
    "# Run verification\n",
    "result = VerificationSuite(spark) \\\n",
    "    .onData(data) \\\n",
    "    .addCheck(check) \\\n",
    "    .run()\n",
    "\n",
    "result_df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
    "\n",
    "unique_id = str(uuid.uuid4())\n",
    "result_df.write.mode(\"overwrite\").json(f\"dbfs:/FileStore/deequ_report_{unique_id}\")\n",
    "\n",
    "# Stop job if failed\n",
    "if result.status != \"Success\":\n",
    "    print(\"Deequ checks failed.\")\n",
    "else:\n",
    "    print(\"Deequ checks passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44e3f835-eaf1-4f28-83c6-fc488cea2a74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "baseline_pdf = pd.DataFrame([\n",
    "    {\"transaction_id\": 9001, \"sales_amount\": 250, \"product_id\": 101, \"store_id\": 1},\n",
    "    {\"transaction_id\": 9002, \"sales_amount\": 300, \"product_id\": 102, \"store_id\": 2},\n",
    "    {\"transaction_id\": 9003, \"sales_amount\": 150, \"product_id\": 103, \"store_id\": 1},\n",
    "    {\"transaction_id\": 9004, \"sales_amount\": 400, \"product_id\": 104, \"store_id\": 3},\n",
    "])\n",
    "\n",
    "current_pdf = data.toPandas()\n",
    "stat, p_value = ks_2samp(baseline_pdf['sales_amount'], current_pdf['sales_amount'])\n",
    "drift_detected = p_value < 0.05\n",
    "\n",
    "drift_schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"column\", StringType(), True),\n",
    "    StructField(\"drift_detected\", BooleanType(), True),\n",
    "    StructField(\"p_value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame with schema\n",
    "drift_df = spark.createDataFrame([\n",
    "    Row(\n",
    "        date=str(date.today()),\n",
    "        column=\"sales_amount\",\n",
    "        drift_detected=bool(drift_detected),\n",
    "        p_value=float(p_value)\n",
    "    )\n",
    "], schema=drift_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "595f93cf-4eca-4b6e-afd4-369d0ebeba7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deequ_results_json = result_df.toPandas().to_dict(orient=\"records\")\n",
    "drift_results_json = [{\n",
    "    \"date\": str(date.today()),\n",
    "    \"column\": \"sales_amount\",\n",
    "    \"ks_statistic\": float(stat),\n",
    "    \"p_value\": float(p_value),\n",
    "    \"drift_detected\": bool(drift_detected)\n",
    "}]\n",
    "\n",
    "final_output = {\n",
    "    \"deequ_results\": deequ_results_json,\n",
    "    \"drift_results\": drift_results_json\n",
    "}\n",
    "\n",
    "final_output_json = json.dumps(final_output, indent=2)\n",
    "\n",
    "# Print for logs\n",
    "print(final_output_json)\n",
    "\n",
    "# Exit for GitHub Actions\n",
    "dbutils.notebook.exit(final_output_json)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5969135148514854,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "day2_casestudy",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}